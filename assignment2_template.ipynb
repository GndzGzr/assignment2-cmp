{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CM3whRHJ_Baf"
   },
   "source": [
    "## **BBM 444 - Programming Assignment 2: HDR Imaging and Tonemapping**\n",
    "\n",
    "* You can add as many cells as you want in-between each question, define any function and variable as needed. However, your code needs to be well-commented.  \n",
    "* Please add Markdown cells to answer the (non-coding) questions in the homework text.\n",
    "* Please be careful about the order of runs of cells. Doing the homework, it is likely that you will be running the cells in different orders, however, they will be evaluated in the order they appear. Hence, please try running the cells in this order before submission to make sure they work.    \n",
    "* Please refer to the homework text for any implementation detail. Though you are somewhat expected to abide by the comments in the below cells, they are mainly just provided for guidance and by no means are they complete and/or descriptive enough. Accordingly, as long as you are not completely off this structure and your work pattern is understandable and traceable, it is fine. For instance, you do not have to implement a particular function within a cell just because the comment directs you to do so. \n",
    "* **Required packages and functions:** numpy, skimage, matplotlib, and cv2 (OpenCV, to read and write HDR files), and you should use the functions provided in the ./ src/cp_assng2.py file of the assignment ZIP archive.\n",
    "* Working with the provided and captured exposure stacks, you will notice that your algorithms will be using a lot of memory. That's why you should be careful about how many of these images you create in your Python code, as otherwise you run the risk of filling up your memory. Further, you need to make sure you use vectorized code that processes all of its pixels in parallel, as trying to process all 25 million pixels one-by-one with a double for loop will take ages.\n",
    "* To be able to display HDR images, multiply your image with an appropriate scaling factor of your own (smaller than 1 if the image is very bright, larger than 1 otherwise), apply gamma encoding, and then use the clip and imshow functions as in Programming Assignment 1. You will likely need to experiment with a few different values for the scaling factor you apply, until you find the one that correctly exposes your image. Otherwise, it may appear very bright or very dark.\n",
    "* There will be a lot of reusing the same functionality, hence implementing the algorithms you are asked with functions (and making appropriate calls when necessary, rather than just code blocks ) is likely to be beneficial.\n",
    "* You can use the function readHDR in the code we provide to open\n",
    "and load the .HDR in Python, as otherwise to open HDR images, you'll need a dedicated viewer.\n",
    "* **This document is also your report. Show your work. Please articulate your arguments well and support them using relevant figures and images.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7McHUzxcdhHn"
   },
   "source": [
    "### *Insert personal info here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2s_G963_aW4"
   },
   "source": [
    "### 1. HDR Imaging (60 points)\n",
    "\n",
    "You are provided with both RAW (with .NEF extensions) and rendered (with .JPG extensions) images in the data folder, both of which are to be used to create HDR images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jgha6LQQElC1"
   },
   "source": [
    "#### 1.1. Develop RAW images (5 points)\n",
    "\n",
    "To convert RAW .NEF images into linear 16-bit .TIFF images, use dcraw and specify the camera's profile for white balancing, high-quality interpolation for demosaicing, and sRGB as the output color space. The correct set of flags for this conversion can be found in dcraw's documentation. Report them in the below cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDsAuka1FXLu"
   },
   "source": [
    "##### ***REPORT dcraw flags here. Double tap to write in this cell.***\n",
    "\n",
    "\n",
    "\"-v\": \"Verbose mode - prints detailed processing information\",\n",
    "\n",
    "\"-w\": \"Use camera white balance (as seen in output 'multipliers 1.058594 1.000000 2.500000 1.000000')\",\n",
    "\n",
    "\"-4\": \"Write 16-bit linear output (for linear TIFF requirement)\",\n",
    "\n",
    "\"-t 0\": \"High-quality interpolation for demosaicing (uses AHD - Adaptive \n",
    "Homogeneity-Directed interpolation)\",\n",
    "\n",
    "\"-T\": \"Write TIFF output instead of PPM\",\n",
    "\n",
    "\"-j\": \"Use camera color profile and convert to sRGB colorspace (as seen in 'Converting to sRGB colorspace')\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OUTPUT\n",
    "\n",
    "Loading NIKON D2X image from data/Lab Booth/_MDF0019.NEF ...\n",
    "Scaling with darkness 0, saturation 4095, and\n",
    "multipliers 1.058594 1.000000 2.500000 1.000000\n",
    "AHD interpolation...\n",
    "Converting to sRGB colorspace...\n",
    "Writing data to data/Lab Booth/_MDF0019.tiff ...\n",
    "\n",
    "\n",
    "Loading NIKON D2X image from data/Lab Booth/_MDF0020.NEF ...\n",
    "Scaling with darkness 0, saturation 4095, and\n",
    "multipliers 1.058594 1.000000 2.500000 1.000000\n",
    "AHD interpolation...\n",
    "Converting to sRGB colorspace...\n",
    "Writing data to data/Lab Booth/_MDF0020.tiff ...\n",
    "\n",
    "\n",
    "Loading NIKON D2X image from data/Lab Booth/_MDF0021.NEF ...\n",
    "Scaling with darkness 0, saturation 4095, and\n",
    "multipliers 1.058594 1.000000 2.500000 1.000000\n",
    "AHD interpolation...\n",
    "Converting to sRGB colorspace...\n",
    "Writing data to data/Lab Booth/_MDF0021.tiff ...\n",
    "\n",
    "\n",
    "Loading NIKON D2X image from data/Lab Booth/_MDF0022.NEF ...\n",
    "Scaling with darkness 0, saturation 4095, and\n",
    "multipliers 1.058594 1.000000 2.500000 1.000000\n",
    "AHD interpolation...\n",
    "Converting to sRGB colorspace...\n",
    "Writing data to data/Lab Booth/_MDF0022.tiff ...\n",
    "\n",
    "\n",
    "Loading NIKON D2X image from data/Lab Booth/_MDF0023.NEF ...\n",
    "Scaling with darkness 0, saturation 4095, and\n",
    "multipliers 1.058594 1.000000 2.500000 1.000000\n",
    "AHD interpolation...\n",
    "Converting to sRGB colorspace...\n",
    "Writing data to data/Lab Booth/_MDF0023.tiff ...\n",
    "\n",
    "\n",
    "Loading NIKON D2X image from data/Lab Booth/_MDF0024.NEF ...\n",
    "Scaling with darkness 0, saturation 4095, and\n",
    "multipliers 1.058594 1.000000 2.500000 1.000000\n",
    "AHD interpolation...\n",
    "Converting to sRGB colorspace...\n",
    "Writing data to data/Lab Booth/_MDF0024.tiff ...\n",
    "\n",
    "\n",
    "Loading NIKON D2X image from data/Lab Booth/_MDF0025.NEF ...\n",
    "Scaling with darkness 0, saturation 4095, and\n",
    "multipliers 1.058594 1.000000 2.500000 1.000000\n",
    "AHD interpolation...\n",
    "Converting to sRGB colorspace...\n",
    "Writing data to data/Lab Booth/_MDF0025.tiff ...\n",
    "\n",
    "\n",
    "Loading NIKON D2X image from data/Lab Booth/_MDF0026.NEF ...\n",
    "Scaling with darkness 0, saturation 4095, and\n",
    "multipliers 1.058594 1.000000 2.500000 1.000000\n",
    "AHD interpolation...\n",
    "Converting to sRGB colorspace...\n",
    "Writing data to data/Lab Booth/_MDF0026.tiff ...\n",
    "\n",
    "\n",
    "Loading NIKON D2X image from data/Lab Booth/_MDF0027.NEF ...\n",
    "Scaling with darkness 0, saturation 4095, and\n",
    "multipliers 1.058594 1.000000 2.500000 1.000000\n",
    "AHD interpolation...\n",
    "Converting to sRGB colorspace...\n",
    "Writing data to data/Lab Booth/_MDF0027.tiff ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oQtnKXbKI4p"
   },
   "source": [
    "#### 1. 2. Weighting Schemes\n",
    "\n",
    "You are expected to implement 4 different weighting schemes, namely, uniform, tent, Gaussian, and photon. (All hold the assumption that the intensity values z $\\in$ [0,1]).\n",
    "\n",
    "\\\\begin{eqnarray}\n",
    "w_{\\text{uniform}} & = & \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            1, & \\;\\, \\text{if}\\;\\, Z_{\\text{min}}\\leq z \\leq Z_{\\text{max}}, \\\\\n",
    "            0, & \\;\\, \\text{otherwise}\n",
    "        \\end{array},\n",
    "    \\right.\\\\\\nonumber\n",
    "w_{\\text{tent}} & = & \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            \\text{min}(z,1-z), & \\;\\, \\text{if}\\;\\, Z_{\\text{min}}\\leq z \\leq Z_{\\text{max}}, \\\\\n",
    "            0, & \\;\\, \\text{otherwise}\n",
    "        \\end{array},\n",
    "    \\right. \\\\\\nonumber\n",
    "w_{\\text{Gaussian}} & = & \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            \\text{exp}\\left(-4\\frac{(z-0.5)^2}{0.5^2}\\right), & \\;\\, \\text{if}\\;\\, Z_{\\text{min}}\\leq z \\leq Z_{\\text{max}}, \\\\\n",
    "            0, & \\;\\, \\text{otherwise}\n",
    "        \\end{array},\n",
    "    \\right. \\\\\\nonumber\n",
    "w_{\\text{photon}} & = & \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            t^k, & \\;\\, \\text{if}\\;\\, Z_{\\text{min}}\\leq z \\leq Z_{\\text{max}}, \\\\\n",
    "            0, & \\;\\, \\text{otherwise}\n",
    "        \\end{array},\n",
    "    \\right.\n",
    "\\end{eqnarray} \n",
    "\n",
    "Though the recommended values for $Z_{\\text{max}}$ and $Z_{\\text{min}}$ are 0.05 and 0.95 respectively, you can experiment with different clipping values. \n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T22:48:38.915687Z",
     "iopub.status.busy": "2025-04-08T22:48:38.915046Z",
     "iopub.status.idle": "2025-04-08T22:48:39.016100Z",
     "shell.execute_reply": "2025-04-08T22:48:39.015478Z",
     "shell.execute_reply.started": "2025-04-08T22:48:38.915669Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"assignments/assignment2-cmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T22:48:42.853363Z",
     "iopub.status.busy": "2025-04-08T22:48:42.852905Z",
     "iopub.status.idle": "2025-04-08T22:48:42.930916Z",
     "shell.execute_reply": "2025-04-08T22:48:42.930444Z",
     "shell.execute_reply.started": "2025-04-08T22:48:42.853345Z"
    },
    "id": "6GChYYV1usDq"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'weighting_schemes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Implement (possibly) functions or a class for these weighting schemes\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mweighting_schemes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WeightingSchemes\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'weighting_schemes'"
     ]
    }
   ],
   "source": [
    "## Implement (possibly) functions or a class for these weighting schemes\n",
    "from weighting_schemes import WeightingSchemes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smGh8isZFqj0"
   },
   "source": [
    "#### 1.3. Linearize rendered images (25 points)\n",
    "\n",
    "Rendered images, being non-linear, need to be linearized before merging into HDR images. For this, we'll use the method by Debevec and Malik [1]. Please refer to the homework text and the paper for any implementation detail.\n",
    "\n",
    "$I^k_{ij}: $ intensity of pixel (i, j) of the $k^{th}$ image and $I^k_{ij} = f(t^kL_{ij}) $, where $I^k_{ij} \\in \\{ 0, ..., 255 \\}$ and $t^k$ the exposure time of image k. f is the non-linearity introduced by the camera's ISP. That is, calculating its inverse, $f^{-1}$, one can get the linear response.\n",
    "\n",
    "Instead of $f^{-1}$, recover the function $g =\\text{log}(f^{-1})$ that maps the pixel values $I^k_{ij}$ to $g(I^k_{ij})=\\text{log}(L_{ij})+\\text{log}(t^k)$. **Notice domain of g is discrete** an takes on values in the range $\\{ 0, ..., 255 \\}$. Hence, the second derivative of g can be approximated via a Laplacian filter.\n",
    "\n",
    "Assuming static scene upon capturing, $L_{ij}$ is constant across all LDR images. That is, to recover g, the least-squares optimization problem in the hw text needs to be solved. \n",
    "\n",
    "*Hint: Solve problem by expressing it in matrix form: $||Av -b ||^2$, where A is a matrix and $\\mathbf{v} = [g; \\text{log} (L_{ij} )]$ are the unknowns. Use one of NumPy’s solvers to recover the unknowns. (See numpy function numpy.linalg.lstsq)* \n",
    "\n",
    "Though, Debevec and Malik [1] recovers a g for each color channel, recovering a single g for all channels suffices for this homework.\n",
    "\n",
    "Plot the function g you recovered and use it to convert non-linear images into linear.\n",
    "\n",
    "**Solving the linear equation, unless downsampled, processing the whole image will cause memory error, ie, you will run out of memory. That's why you need to downsample the image I with I[::N, ::N], for some N . We recommend using N = 200. In general it is advisable to use downscaled images during the debugging process of your code to speed up the development process. After ensuring the correctness of the code, the final results can be obtained by running it on the full-resolution image.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T22:48:46.282200Z",
     "iopub.status.busy": "2025-04-08T22:48:46.281914Z",
     "iopub.status.idle": "2025-04-08T22:48:46.285755Z",
     "shell.execute_reply": "2025-04-08T22:48:46.285087Z",
     "shell.execute_reply.started": "2025-04-08T22:48:46.282182Z"
    },
    "id": "VdrQSIYcFWFd"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'response_calibration'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Implement a solver (perhaps a function) for the optimization problem and recovering g\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mresponse_calibration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ResponseCalibration\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'response_calibration'"
     ]
    }
   ],
   "source": [
    "## Implement a solver (perhaps a function) for the optimization problem and recovering g\n",
    "from response_calibration import ResponseCalibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqpFOhrwv4g9"
   },
   "source": [
    "#### 1.4. Merge exposure stack into HDR image (30 points)\n",
    "Given a set of k LDR linear images corresponding to different exposures $t^k$, we can merge them into an HDR image either in the linear or in the logarithmic domain, where the motivation for the former is physical accuracy, whereas, that for the latter is human visual perception.\n",
    "\n",
    "Please see the homework text for both algorithms. \n",
    "\n",
    "**Merging multiple LDR images, some pixels may not have well-exposed values, which makes the sum of weights in the equations' denominators zero. For over-exposed pixels, assign the maximum valid pixel value, and for under-exposed pixels, assign the minimum valid pixel value.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T22:48:48.253697Z",
     "iopub.status.busy": "2025-04-08T22:48:48.253452Z",
     "iopub.status.idle": "2025-04-08T22:48:48.257153Z",
     "shell.execute_reply": "2025-04-08T22:48:48.256547Z",
     "shell.execute_reply.started": "2025-04-08T22:48:48.253679Z"
    },
    "id": "vfHCTLZYwjeO"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hdr_merging'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhdr_merging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HDRMerger\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hdr_merging'"
     ]
    }
   ],
   "source": [
    "from hdr_merging import HDRMerger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_w6EYdzAenGM"
   },
   "source": [
    "#### 1.4. Experiment\n",
    "\n",
    "You have 2 sets of images (RAW and rendered), 2 merging schemes (linear and logarithmic), and 4 weighting schemes (uniform, tent, Gaussian, and photon-noise optimal), which, in total, makes 16 different HDR images. Additionally, you will need to tune the regularizer hyperparameter $\\lambda$.  \n",
    "\n",
    "Select one out of the sixteen HDR images you created. You can select, for\n",
    "example, the one that you find the most aesthetically pleasing. Make sure to comment on why you selected the image. Note that, as you have not yet tonemapped your HDR images, if you display them directly they will not look very nice; see “Hints and Information”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T22:49:46.922313Z",
     "iopub.status.busy": "2025-04-08T22:49:46.922087Z",
     "iopub.status.idle": "2025-04-08T22:49:46.925107Z",
     "shell.execute_reply": "2025-04-08T22:49:46.924660Z",
     "shell.execute_reply.started": "2025-04-08T22:49:46.922295Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hdr_experiments'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhdr_experiments\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hdr_experiments'"
     ]
    }
   ],
   "source": [
    "from hdr_experiments import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T22:49:00.179294Z",
     "iopub.status.busy": "2025-04-08T22:49:00.178530Z",
     "iopub.status.idle": "2025-04-08T22:49:00.187730Z",
     "shell.execute_reply": "2025-04-08T22:49:00.187247Z",
     "shell.execute_reply.started": "2025-04-08T22:49:00.179270Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_MDF0021.NEF',\n",
       " '_MDF0019.NEF',\n",
       " '_MDF0023.NEF',\n",
       " '_MDF0024.NEF',\n",
       " '_MDF0027.NEF',\n",
       " '_MDF0021.tiff',\n",
       " '_MDF0027.tiff',\n",
       " '_MDF0020.NEF',\n",
       " '_MDF0026.NEF',\n",
       " '_MDF0019.tiff',\n",
       " '_MDF0026.tiff',\n",
       " '_MDF0023.tiff',\n",
       " '_MDF0020.tiff',\n",
       " '_MDF0021.jpg',\n",
       " '_MDF0027.jpg',\n",
       " '_MDF0022.tiff',\n",
       " '_MDF0024.tiff',\n",
       " '_MDF0023.jpg',\n",
       " '_MDF0022.jpg',\n",
       " '_MDF0020.jpg',\n",
       " '_MDF0025.tiff',\n",
       " '_MDF0026.jpg',\n",
       " '_MDF0019.jpg',\n",
       " '_MDF0024.jpg',\n",
       " '_MDF0025.NEF',\n",
       " '_MDF0022.NEF',\n",
       " '_MDF0025.jpg']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T22:49:50.858899Z",
     "iopub.status.busy": "2025-04-08T22:49:50.858676Z",
     "iopub.status.idle": "2025-04-08T22:49:50.883976Z",
     "shell.execute_reply": "2025-04-08T22:49:50.883337Z",
     "shell.execute_reply.started": "2025-04-08T22:49:50.858837Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "An error occurred: No module named 'hdr_experiments'\n",
      "\n",
      "Detailed error information:\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1509/1639103817.py\", line 5, in <module>\n",
      "    from hdr_experiments import run_experiments, visualize_results\n",
      "ModuleNotFoundError: No module named 'hdr_experiments'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Import required modules\n",
    "    import os\n",
    "    import traceback\n",
    "    from hdr_experiments import run_experiments, visualize_results\n",
    "    \n",
    "    print(\"Starting experiments...\")\n",
    "    \n",
    "    # Run the experiments (path handling is already done in hdr_experiments.py)\n",
    "    results = run_experiments()\n",
    "    \n",
    "    # Visualize the results\n",
    "    print(\"\\nGenerating visualizations...\")\n",
    "    visualize_results(results)\n",
    "    \n",
    "    print(\"\\nExperiments completed successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred: {str(e)}\")\n",
    "    print(\"\\nDetailed error information:\")\n",
    "    print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "UrYePwj0qwd1"
   },
   "outputs": [],
   "source": [
    "## Plot g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kkqWtjNvfo8"
   },
   "source": [
    "#### *Choose one of the HDR images you have created, commenting on the reason for your choice.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IRddaevXyAmi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TyFwhD_ix9Ne"
   },
   "source": [
    "*Store the resulting HDR images as \\texttt{.HDR} files, which is an open source high dynamic range file format. (See the provided function **`writeHDR`**  ./src/cp\\_assgn2.py)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VlV5O34kx7z3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tN_ttJ2yGZ9h"
   },
   "source": [
    "## 2. Color correction and white balancing (20 points)\n",
    "\n",
    "For this part, you are expected to use the **read_colorchecker_gm()** function provided in **./src/cp_assgn2.py**., which **returns a 4x6 matrix with sRGB linear values of the Greatg-Macbeth color checker.** \n",
    "\n",
    "\n",
    "\n",
    "1. For each color checker patch, crop a square that is fully contained within the patch. (See mat-plotlib function matplotlib.pyplot.ginput for interactively recording image coordinates). Make sure to store the coordinates of these cropped squares, so that you can re-use them. Use the resulting 24 crops to compute average RGB coordinates for each of the color checker’s 24 patches\n",
    "2. Convert these computed RGB coordinates into homogeneous 4 × 1 coordinates, by appending a 1 as their fourth coordinate.\n",
    "3. Solve a least-squares problem to compute an affine transformation, mapping the measured to the ground-truth  homogeneous coordinates.\n",
    "4. Apply the computed affine transform to your original RGB HDR image. Note that the\n",
    "transformed image may have some negative values, which you should clip to 0.\n",
    "5. Finally, apply an additional white balancing transform (i.e., multiply each channel with a scalar), so that the RGB coordinates of patch 4 are equal to each other. This is analogous to the manual white balancing in Programming Assignment 1, where now we use patch 4 as the white object in the scene.\n",
    "\n",
    "Store the color corrected and white balanced HDR image in an .HDR file. You should now have two HDR images total: The one from Part 1 that has not been color-corrected, and the one you just created. Compare the color-corrected image with the original, and discuss which one you like the best.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "aKCUCCD6sULM"
   },
   "outputs": [],
   "source": [
    "# for each color patch: get 24 crops\n",
    "def crop_color_patches(image, interactive=True, saved_coordinates=None):\n",
    "    \"\"\"\n",
    "    Crop 24 patches from color checker, either interactively or using saved coordinates.\n",
    "    \n",
    "    Args:\n",
    "        image (numpy.ndarray): Input HDR image\n",
    "        interactive (bool): Whether to use interactive selection or saved coordinates\n",
    "        saved_coordinates (list): List of (x, y, size) tuples for each patch if not interactive\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (patches, coordinates) where patches is a list of 24 cropped regions and\n",
    "              coordinates is a list of (x, y, size) tuples\n",
    "    \"\"\"\n",
    "    patches = []\n",
    "    coordinates = []\n",
    "    \n",
    "    if interactive:\n",
    "        # Display the image for interactive selection\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(np.clip(image, 0, 1))  # Clip for display purposes\n",
    "        plt.title(\"Select the center of each color patch (24 points total)\")\n",
    "        plt.axis('on')\n",
    "        \n",
    "        # Get 24 points from user interaction\n",
    "        points = plt.ginput(24, timeout=0)\n",
    "        plt.close()\n",
    "        \n",
    "        # Define patch size (you may need to adjust this)\n",
    "        patch_size = min(image.shape[0], image.shape[1]) // 20\n",
    "        \n",
    "        # Extract patches\n",
    "        for x, y in points:\n",
    "            x, y = int(x), int(y)\n",
    "            half_size = patch_size // 2\n",
    "            \n",
    "            # Ensure the patch is within image boundaries\n",
    "            x_start = max(0, x - half_size)\n",
    "            y_start = max(0, y - half_size)\n",
    "            x_end = min(image.shape[1], x + half_size)\n",
    "            y_end = min(image.shape[0], y + half_size)\n",
    "            \n",
    "            patch = image[y_start:y_end, x_start:x_end]\n",
    "            patches.append(patch)\n",
    "            coordinates.append((x, y, patch_size))\n",
    "    else:\n",
    "        # Use saved coordinates\n",
    "        if saved_coordinates is None or len(saved_coordinates) != 24:\n",
    "            raise ValueError(\"Need exactly 24 saved coordinates\")\n",
    "        \n",
    "        for x, y, size in saved_coordinates:\n",
    "            half_size = size // 2\n",
    "            x_start = max(0, x - half_size)\n",
    "            y_start = max(0, y - half_size)\n",
    "            x_end = min(image.shape[1], x + half_size)\n",
    "            y_end = min(image.shape[0], y + half_size)\n",
    "            \n",
    "            patch = image[y_start:y_end, x_start:x_end]\n",
    "            patches.append(patch)\n",
    "            coordinates.append((x, y, size))\n",
    "    \n",
    "    return patches, coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3BErFZqssfNM"
   },
   "outputs": [],
   "source": [
    "# compute average RGB coordinates for each of the color checker’s 24 patches and convert them to 4x1 coords by adding a 1\n",
    "def compute_average_rgb(patches):\n",
    "    \"\"\"\n",
    "    Compute average RGB values for each patch.\n",
    "    \n",
    "    Args:\n",
    "        patches (list): List of 24 cropped patch regions\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: 24x3 array of average RGB values\n",
    "    \"\"\"\n",
    "    avg_rgb = []\n",
    "    \n",
    "    for patch in patches:\n",
    "        # Compute average RGB for each patch\n",
    "        avg_color = np.mean(patch, axis=(0, 1))\n",
    "        avg_rgb.append(avg_color)\n",
    "    \n",
    "    return np.array(avg_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "RZnlNW-isk2i"
   },
   "outputs": [],
   "source": [
    "# compute an affine transformation\n",
    "def convert_to_homogeneous(rgb_values):\n",
    "    \"\"\"\n",
    "    Convert RGB values to homogeneous coordinates by appending 1.\n",
    "    \n",
    "    Args:\n",
    "        rgb_values (numpy.ndarray): Nx3 array of RGB values\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Nx4 array of homogeneous coordinates\n",
    "    \"\"\"\n",
    "    # Add a column of ones to make homogeneous coordinates\n",
    "    ones = np.ones((rgb_values.shape[0], 1))\n",
    "    return np.hstack((rgb_values, ones))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nxmA_5kRsvtb"
   },
   "outputs": [],
   "source": [
    "# Apply the computed affine transform to your original RGB HDR image\n",
    "def compute_affine_transform(source_coords, target_coords):\n",
    "    \"\"\"\n",
    "    Compute affine transformation matrix using least squares.\n",
    "    \n",
    "    Args:\n",
    "        source_coords (numpy.ndarray): Nx4 homogeneous coordinates of measured values\n",
    "        target_coords (numpy.ndarray): Nx3 ground truth RGB values\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: 4x3 affine transformation matrix\n",
    "    \"\"\"\n",
    "    # Solve for each channel separately\n",
    "    transformation = np.zeros((4, 3))\n",
    "    \n",
    "    for i in range(3):  # For R, G, B channels\n",
    "        # Solve least squares: source_coords * X = target_coords[:, i]\n",
    "        x, residuals, rank, s = np.linalg.lstsq(source_coords, target_coords[:, i], rcond=None)\n",
    "        transformation[:, i] = x\n",
    "    \n",
    "    return transformation\n",
    "\n",
    "def apply_affine_transform(image, transform):\n",
    "    \"\"\"\n",
    "    Apply affine transformation to an HDR image.\n",
    "    \n",
    "    Args:\n",
    "        image (numpy.ndarray): Input HDR image\n",
    "        transform (numpy.ndarray): 4x3 affine transformation matrix\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Color-corrected HDR image\n",
    "    \"\"\"\n",
    "    # Reshape image to 2D array of pixels\n",
    "    h, w, c = image.shape\n",
    "    pixels = image.reshape(-1, c)\n",
    "    \n",
    "    # Convert to homogeneous coordinates\n",
    "    homogeneous = np.hstack((pixels, np.ones((pixels.shape[0], 1))))\n",
    "    \n",
    "    # Apply transformation\n",
    "    corrected = np.dot(homogeneous, transform)\n",
    "    \n",
    "    # Reshape back to image\n",
    "    corrected_image = corrected.reshape(h, w, c)\n",
    "    \n",
    "    # Clip negative values\n",
    "    corrected_image = np.maximum(corrected_image, 0)\n",
    "    \n",
    "    return corrected_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "AXQvQa0ysynX"
   },
   "outputs": [],
   "source": [
    "# Apply white balancing\n",
    "def apply_white_balance(image, patch4_avg):\n",
    "    \"\"\"\n",
    "    Apply white balancing transform so that the RGB coordinates of patch 4 are equal.\n",
    "    \n",
    "    Args:\n",
    "        image (numpy.ndarray): Input HDR image\n",
    "        patch4_avg (numpy.ndarray): Average RGB value of patch 4 (white patch)\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: White-balanced HDR image\n",
    "    \"\"\"\n",
    "    # Get the maximum value across channels of patch 4\n",
    "    max_val = np.max(patch4_avg)\n",
    "    \n",
    "    # Compute scaling factors to make R=G=B for patch 4\n",
    "    scaling = max_val / patch4_avg\n",
    "    \n",
    "    # Apply scaling to entire image\n",
    "    balanced_image = np.zeros_like(image)\n",
    "    for i in range(3):\n",
    "        balanced_image[:, :, i] = image[:, :, i] * scaling[i]\n",
    "    \n",
    "    return balanced_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "WPS-KbWAs3MY"
   },
   "outputs": [],
   "source": [
    "# Store the color corrected HDR image\n",
    "def save_hdr_image(image, filename):\n",
    "    \"\"\"\n",
    "    Save an HDR image to an HDR file.\n",
    "    \n",
    "    Args:\n",
    "        image (numpy.ndarray): HDR image to save\n",
    "        filename (str): Output filename\n",
    "    \"\"\"\n",
    "    # OpenCV expects BGR order for HDR files\n",
    "    bgr_image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(filename, bgr_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_correct_hdr_image(hdr_image, color_checker_values, interactive=True, saved_coordinates=None):\n",
    "    \"\"\"\n",
    "    Complete color correction and white balancing pipeline.\n",
    "    \n",
    "    Args:\n",
    "        hdr_image (numpy.ndarray): Input HDR image\n",
    "        color_checker_values (numpy.ndarray): 4x6x3 ground truth values from color checker\n",
    "        interactive (bool): Whether to select patches interactively\n",
    "        saved_coordinates (list): Previously saved coordinates if not interactive\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (color_corrected_image, white_balanced_image, coordinates)\n",
    "    \"\"\"\n",
    "    # Reshape color checker values to 24x3\n",
    "    ground_truth = color_checker_values.reshape(24, 3)\n",
    "    \n",
    "    # 1. Crop color patches\n",
    "    patches, coordinates = crop_color_patches(hdr_image, interactive, saved_coordinates)\n",
    "    \n",
    "    # 2. Compute average RGB for each patch\n",
    "    avg_rgb = compute_average_rgb(patches)\n",
    "    \n",
    "    # 3. Convert to homogeneous coordinates\n",
    "    homogeneous_coords = convert_to_homogeneous(avg_rgb)\n",
    "    \n",
    "    # 4. Compute affine transformation\n",
    "    transform = compute_affine_transform(homogeneous_coords, ground_truth)\n",
    "    \n",
    "    # 5. Apply affine transformation\n",
    "    color_corrected = apply_affine_transform(hdr_image, transform)\n",
    "    \n",
    "    # 6. Apply white balancing (patch 4 is the 4th patch in the first row, index 3)\n",
    "    patch4_avg = avg_rgb[3]\n",
    "    white_balanced = apply_white_balance(color_corrected, patch4_avg)\n",
    "    \n",
    "    return color_corrected, white_balanced, coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKdIZ53-tGMT"
   },
   "source": [
    "##### *Compare the color-corrected and the original image here, discussing which one you like better.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "tApftMFvv4Hl"
   },
   "outputs": [],
   "source": [
    "\n",
    "def visualize_color_correction(original, color_corrected, white_balanced):\n",
    "    \"\"\"\n",
    "    Visualize original, color-corrected, and white-balanced images for comparison.\n",
    "    \n",
    "    Args:\n",
    "        original (numpy.ndarray): Original HDR image\n",
    "        color_corrected (numpy.ndarray): Color-corrected HDR image\n",
    "        white_balanced (numpy.ndarray): White-balanced HDR image\n",
    "    \"\"\"\n",
    "    # Display function for HDR images\n",
    "    def display_hdr(img):\n",
    "        # Simple tone mapping for display\n",
    "        gamma = 2.2\n",
    "        img_display = np.clip(img, 0, None)\n",
    "        img_display = (img_display / (np.max(img_display) + 1e-6))**(1/gamma)\n",
    "        return np.clip(img_display, 0, 1)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    axes[0].imshow(display_hdr(original))\n",
    "    axes[0].set_title('Original HDR')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(display_hdr(color_corrected))\n",
    "    axes[1].set_title('Color Corrected')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    axes[2].imshow(display_hdr(white_balanced))\n",
    "    axes[2].set_title('White Balanced')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WupgicoMtd35"
   },
   "source": [
    "### 3. Photographic tonemapping (20 points)\n",
    "\n",
    "You need to tonemap the HDR image you like better at the end of the last part for displaying purposes.\n",
    "You will implement the tonemapping operator proposed by Reinhard et al. [2].\n",
    "For implementation details, refer to the homework text and the paper.\n",
    "\n",
    "*You may get better results by using the same scalars for all three channels. You can do this by using pixels from all three channels in the equations.*\n",
    "\n",
    "$I_{white} = B. max_{i,j}(I_{ij,HDR})$,\n",
    "\n",
    "$I_{i,j,HDR} = \\frac{K}{I_{m,HDR}}$\n",
    "\n",
    "$I_{m,HDR} = exp(1/N∑_{i,j}log(I_{ij,HDR} + ϵ))$ **Equation (10)**\n",
    "\n",
    "The parameter K is the key, and determines how bright or dark the resulting tonemapped rendition is. The parameter B is the burn, and can be used to  suppress the contrast of the result. Finally, N is the number of pixels, and ε is a small constant to avoid the singularity of the logarithm function at 0. \n",
    "\n",
    "**Even with tonemapping, your images may appear too dark. After tonemapping, you need to apply gamma encoding for images to be displayed correctly.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYQQmvmJtUOt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l7xsVmilv4nS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDarnHvUv5a7"
   },
   "source": [
    "### 4. (Bonus)  Create and tonemap your own HDR photos (50 points)\n",
    "\n",
    "* Apply your implementation on a photograph you have taken. For this, you need to choose a scene with high dynamic range. See the hints section in the homework text for the camera settings.\n",
    "* Once you select the scene, capture exposure stacks in RAW and JPEG formats. We suggest using exposures that are equally spaced in the logarithmic domain. For example, start with some very low base exposure, and then use exposures that are 2× the base, 4×, 8×, and so on.\n",
    "* Use the exposure stacks you captured to create two HDR images, one from the RAW and one from the JPEG images. Store these images in .HDR format. You do not need color calibration. \n",
    "*Then, process these images using the tonemapping algorithms you implemented in Part 3 (photographic, in RGB or luminance-only). Experiment with different parameters, show a few representative tonemaps, discuss your results, and determine which result you like the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "QSXyrydrAMSV"
   },
   "outputs": [],
   "source": [
    "## Load your images \n",
    "\n",
    "\n",
    "def load_exposure_stack(directory, file_format):\n",
    "    \"\"\"\n",
    "    Load an exposure stack of images from a directory.\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Directory containing exposure stack images\n",
    "        file_format (str): File format ('RAW', 'JPEG', 'NEF', 'JPG', etc.)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (images, exposure_times) where images is a list of image arrays\n",
    "               and exposure_times is a list of corresponding exposure times\n",
    "    \"\"\"\n",
    "    # Get image file paths\n",
    "    if file_format.upper() == 'RAW' or file_format.upper() == 'NEF':\n",
    "        pattern = os.path.join(directory, f'*.NEF')\n",
    "    else:  # JPEG/JPG\n",
    "        pattern = os.path.join(directory, f'*.jpg')\n",
    "        if not glob.glob(pattern):\n",
    "            pattern = os.path.join(directory, f'*.jpeg')\n",
    "    \n",
    "    # Get sorted list of files\n",
    "    file_paths = sorted(glob.glob(pattern))\n",
    "    \n",
    "    if len(file_paths) == 0:\n",
    "        raise ValueError(f\"No {file_format} files found in {directory}\")\n",
    "    \n",
    "    print(f\"Loading {len(file_paths)} {file_format} images...\")\n",
    "    \n",
    "    # Load images\n",
    "    images = []\n",
    "    for file_path in file_paths:\n",
    "        print(f\"  Loading {os.path.basename(file_path)}\")\n",
    "        if file_format.upper() == 'RAW' or file_format.upper() == 'NEF':\n",
    "            # For RAW files, use cv2.imread with ANYDEPTH flag\n",
    "            img = cv2.imread(file_path, cv2.IMREAD_ANYDEPTH | cv2.IMREAD_ANYCOLOR)\n",
    "        else:\n",
    "            # For JPEG/JPG files\n",
    "            img = cv2.imread(file_path)\n",
    "        \n",
    "        # Convert BGR to RGB\n",
    "        if img.ndim == 3 and img.shape[2] == 3:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "        # Add to list\n",
    "        images.append(img)\n",
    "    \n",
    "    # Try to extract exposure times from EXIF data\n",
    "    # For simplicity, we'll use a doubling sequence if EXIF data is not available\n",
    "    try:\n",
    "        import exifread\n",
    "        exposure_times = []\n",
    "        for file_path in file_paths:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                tags = exifread.process_file(f)\n",
    "                if 'EXIF ExposureTime' in tags:\n",
    "                    exp_time = tags['EXIF ExposureTime'].values[0]\n",
    "                    # Convert fraction to float\n",
    "                    if isinstance(exp_time, exifread.utils.Ratio):\n",
    "                        exp_time = float(exp_time.num) / float(exp_time.den)\n",
    "                    else:\n",
    "                        exp_time = float(exp_time)\n",
    "                    exposure_times.append(exp_time)\n",
    "                else:\n",
    "                    # Fall back to doubling sequence\n",
    "                    raise ValueError(\"ExposureTime tag not found\")\n",
    "    except (ImportError, ValueError):\n",
    "        # If exifread not installed or EXIF data not available, use doubling sequence\n",
    "        print(\"Could not extract exposure times from EXIF data. Using doubling sequence...\")\n",
    "        exposure_times = [2**i for i in range(len(images))]\n",
    "    \n",
    "    print(f\"Exposure times: {exposure_times}\")\n",
    "    \n",
    "    return images, exposure_times\n",
    "\n",
    "\n",
    "def preprocess_raw_images(raw_images):\n",
    "    \"\"\"\n",
    "    Preprocess RAW images for HDR merging (normalization, etc.).\n",
    "    \n",
    "    Args:\n",
    "        raw_images (list): List of RAW image arrays\n",
    "        \n",
    "    Returns:\n",
    "        list: Preprocessed images ready for HDR merging\n",
    "    \"\"\"\n",
    "    processed_images = []\n",
    "    \n",
    "    for img in raw_images:\n",
    "        # Convert to float32\n",
    "        img_float = img.astype(np.float32)\n",
    "        \n",
    "        # Normalize to [0, 1] range\n",
    "        if img_float.max() > 0:\n",
    "            img_float = img_float / img_float.max()\n",
    "        \n",
    "        processed_images.append(img_float)\n",
    "    \n",
    "    return processed_images\n",
    "\n",
    "def preprocess_jpeg_images(jpeg_images, exposure_times):\n",
    "    \"\"\"\n",
    "    Preprocess JPEG images for HDR merging (linearization, etc.).\n",
    "    \n",
    "    Args:\n",
    "        jpeg_images (list): List of JPEG image arrays\n",
    "        exposure_times (list): List of exposure times\n",
    "        \n",
    "    Returns:\n",
    "        list: Preprocessed (linearized) images ready for HDR merging\n",
    "    \"\"\"\n",
    "    # Normalize JPEG images to [0, 1]\n",
    "    normalized_images = [img.astype(np.float32) / 255.0 for img in jpeg_images]\n",
    "    \n",
    "    # Recover camera response function\n",
    "    calibration = ResponseCalibration(normalized_images, exposure_times)\n",
    "    g = calibration.solve_g(n_samples=100)\n",
    "    \n",
    "    # Linearize images\n",
    "    linearized_images = []\n",
    "    for img in normalized_images:\n",
    "        # Convert to uint8 for indexing into g\n",
    "        img_uint8 = np.clip(img * 255, 0, 255).astype(np.uint8)\n",
    "        linear_img = calibration.linearize_image(img_uint8, g)\n",
    "        linearized_images.append(linear_img)\n",
    "    \n",
    "    return linearized_images, g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "j428FxccARwb"
   },
   "outputs": [],
   "source": [
    "## Create two HDR images (one from RAW and one from JPEG imgs) using the above implemented algorithms\n",
    "def create_hdr_image(images, exposure_times, weighting_method='gaussian', merging_method='logarithmic'):\n",
    "    \"\"\"\n",
    "    Create an HDR image from an exposure stack.\n",
    "    \n",
    "    Args:\n",
    "        images (list): List of preprocessed images\n",
    "        exposure_times (list): List of exposure times\n",
    "        weighting_method (str): Weighting method ('uniform', 'tent', 'gaussian', 'photon')\n",
    "        merging_method (str): Merging method ('linear', 'logarithmic')\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: HDR image\n",
    "    \"\"\"\n",
    "    # Create HDR merger\n",
    "    merger = HDRMerger(weighting_method=weighting_method)\n",
    "    \n",
    "    # Merge images using specified method\n",
    "    if merging_method == 'linear':\n",
    "        hdr_image = merger.linear_merging(images, exposure_times)\n",
    "    else:\n",
    "        hdr_image = merger.logarithmic_merging(images, exposure_times)\n",
    "    \n",
    "    return hdr_image\n",
    "\n",
    "def save_hdr_image(hdr_image, filename):\n",
    "    \"\"\"\n",
    "    Save HDR image to file.\n",
    "    \n",
    "    Args:\n",
    "        hdr_image (numpy.ndarray): HDR image\n",
    "        filename (str): Output filename\n",
    "    \"\"\"\n",
    "    # Convert RGB to BGR for OpenCV\n",
    "    bgr_image = cv2.cvtColor(hdr_image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Save as HDR file\n",
    "    cv2.imwrite(filename, bgr_image)\n",
    "    print(f\"Saved HDR image to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "9JNSPSPDArbU"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (801502394.py, line 75)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[19], line 75\u001b[0;36m\u001b[0m\n\u001b[0;31m    ef find_best_tone_mapping_params(hdr_image, method='luminance'):\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## Tonemap the images with the above-implemented algorithms (in Part 3). Experiment with different parameters. Show some tonemaps\n",
    "\n",
    "def apply_tone_mapping(hdr_image, method='luminance', key=0.18, burn=0.85, gamma=2.2):\n",
    "    \"\"\"\n",
    "    Apply tone mapping to HDR image.\n",
    "    \n",
    "    Args:\n",
    "        hdr_image (numpy.ndarray): HDR image\n",
    "        method (str): Tone mapping method ('rgb' or 'luminance')\n",
    "        key (float): Key value (brightness)\n",
    "        burn (float): Burn value (highlight compression)\n",
    "        gamma (float): Gamma value for gamma correction\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Tone mapped LDR image\n",
    "    \"\"\"\n",
    "    # Create tone mapper with specified parameters\n",
    "    tone_mapper = ToneMapper(key=key, burn=burn)\n",
    "    \n",
    "    # Apply tone mapping\n",
    "    ldr_image = tone_mapper.tone_map(hdr_image, method=method, apply_gamma_correction=True)\n",
    "    \n",
    "    return ldr_image\n",
    "\n",
    "def experiment_with_tone_mapping(hdr_image, output_dir='tone_mapping_results'):\n",
    "    \"\"\"\n",
    "    Experiment with different tone mapping parameters and save results.\n",
    "    \n",
    "    Args:\n",
    "        hdr_image (numpy.ndarray): HDR image\n",
    "        output_dir (str): Directory to save results\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create tone mapper\n",
    "    tone_mapper = ToneMapper()\n",
    "    \n",
    "    # Parameters to experiment with\n",
    "    methods = ['luminance', 'rgb']\n",
    "    key_values = [0.09, 0.18, 0.36]\n",
    "    burn_values = [0.7, 0.85, 1.0]\n",
    "    \n",
    "    # Create a figure to display results\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    idx = 1\n",
    "    for method in methods:\n",
    "        for key in key_values:\n",
    "            for burn in burn_values:\n",
    "                # Set parameters\n",
    "                tone_mapper.key = key\n",
    "                tone_mapper.burn = burn\n",
    "                \n",
    "                # Apply tone mapping\n",
    "                ldr_image = tone_mapper.tone_map(hdr_image, method=method)\n",
    "                \n",
    "                # Save image\n",
    "                filename = f\"{output_dir}/tonemap_{method}_key{key}_burn{burn}.png\"\n",
    "                tone_mapper.save_tone_mapped_image(ldr_image, filename)\n",
    "                \n",
    "                # Add to figure\n",
    "                plt.subplot(len(methods), len(key_values) * len(burn_values), idx)\n",
    "                plt.imshow(ldr_image)\n",
    "                plt.title(f\"{method}, key={key}, burn={burn}\")\n",
    "                plt.axis('off')\n",
    "                idx += 1\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/tone_mapping_comparison.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "ef find_best_tone_mapping_params(hdr_image, method='luminance'):\n",
    "    \"\"\"\n",
    "    Interactive tool to find the best tone mapping parameters for an HDR image.\n",
    "    \n",
    "    Args:\n",
    "        hdr_image (numpy.ndarray): HDR image\n",
    "        method (str): Tone mapping method ('rgb' or 'luminance')\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Best parameters (key, burn, gamma)\n",
    "    \"\"\"\n",
    "    # Create figure\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Create tone mapper\n",
    "    tone_mapper = ToneMapper()\n",
    "    \n",
    "    # Initial parameters\n",
    "    key = 0.18\n",
    "    burn = 0.85\n",
    "    gamma = 2.2\n",
    "    \n",
    "    # Apply tone mapping with initial parameters\n",
    "    ldr_image = tone_mapper.tone_map(hdr_image, method=method)\n",
    "    \n",
    "    # Display image\n",
    "    plt.imshow(ldr_image)\n",
    "    plt.title(f\"Method: {method}, Key: {key:.2f}, Burn: {burn:.2f}, Gamma: {gamma:.2f}\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Get user input for parameters\n",
    "    print(\"\\nEnter parameters (press Enter to keep current value):\")\n",
    "    \n",
    "    try:\n",
    "        key_input = input(f\"Key (current={key:.2f}): \")\n",
    "        if key_input:\n",
    "            key = float(key_input)\n",
    "        \n",
    "        burn_input = input(f\"Burn (current={burn:.2f}): \")\n",
    "        if burn_input:\n",
    "            burn = float(burn_input)\n",
    "        \n",
    "        gamma_input = input(f\"Gamma (current={gamma:.2f}): \")\n",
    "        if gamma_input:\n",
    "            gamma = float(gamma_input)\n",
    "    except ValueError:\n",
    "        print(\"Invalid input, using current values.\")\n",
    "    \n",
    "    # Update tone mapper with new parameters\n",
    "    tone_mapper.key = key\n",
    "    tone_mapper.burn = burn\n",
    "    \n",
    "    # Apply tone mapping with new parameters\n",
    "    ldr_image = tone_mapper.tone_map(hdr_image, method=method)\n",
    "    \n",
    "    # Display final image\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(ldr_image)\n",
    "    plt.title(f\"Method: {method}, Key: {key:.2f}, Burn: {burn:.2f}, Gamma: {gamma:.2f}\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Final parameters: Key={key:.2f}, Burn={burn:.2f}, Gamma={gamma:.2f}\")\n",
    "    \n",
    "    return key, burn, gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_raw_jpeg_hdr(raw_hdr, jpeg_hdr, output_file='hdr_comparison.png'):\n",
    "    \"\"\"\n",
    "    Compare HDR images created from RAW and JPEG.\n",
    "    \n",
    "    Args:\n",
    "        raw_hdr (numpy.ndarray): HDR image from RAW\n",
    "        jpeg_hdr (numpy.ndarray): HDR image from JPEG\n",
    "        output_file (str): Output filename for comparison image\n",
    "    \"\"\"\n",
    "    # Create tone mapper for display\n",
    "    tone_mapper = ToneMapper()\n",
    "    \n",
    "    # Tone map both images with the same parameters\n",
    "    raw_ldr = tone_mapper.tone_map(raw_hdr)\n",
    "    jpeg_ldr = tone_mapper.tone_map(jpeg_hdr)\n",
    "    \n",
    "    # Create figure for comparison\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(raw_ldr)\n",
    "    plt.title('HDR from RAW')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(jpeg_ldr)\n",
    "    plt.title('HDR from JPEG')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bonus_workflow_example():\n",
    "    \"\"\"\n",
    "    Example workflow for the bonus task.\n",
    "    \"\"\"\n",
    "    # 1. Load exposure stack\n",
    "    print(\"Step 1: Load exposure stacks\")\n",
    "    raw_images, raw_exposures = load_exposure_stack('your_raw_directory', 'RAW')\n",
    "    jpeg_images, jpeg_exposures = load_exposure_stack('your_jpeg_directory', 'JPEG')\n",
    "    \n",
    "    # 2. Preprocess images\n",
    "    print(\"\\nStep 2: Preprocess images\")\n",
    "    processed_raw = preprocess_raw_images(raw_images)\n",
    "    processed_jpeg, response_curve = preprocess_jpeg_images(jpeg_images, jpeg_exposures)\n",
    "    \n",
    "    # 3. Create HDR images\n",
    "    print(\"\\nStep 3: Create HDR images\")\n",
    "    raw_hdr = create_hdr_image(processed_raw, raw_exposures, 'gaussian', 'logarithmic')\n",
    "    jpeg_hdr = create_hdr_image(processed_jpeg, jpeg_exposures, 'gaussian', 'logarithmic')\n",
    "    \n",
    "    # 4. Save HDR images\n",
    "    print(\"\\nStep 4: Save HDR images\")\n",
    "    save_hdr_image(raw_hdr, 'raw_hdr.hdr')\n",
    "    save_hdr_image(jpeg_hdr, 'jpeg_hdr.hdr')\n",
    "    \n",
    "    # 5. Compare HDR images\n",
    "    print(\"\\nStep 5: Compare HDR images\")\n",
    "    compare_raw_jpeg_hdr(raw_hdr, jpeg_hdr)\n",
    "    \n",
    "    # 6. Experiment with tone mapping\n",
    "    print(\"\\nStep 6: Experiment with tone mapping\")\n",
    "    experiment_with_tone_mapping(raw_hdr, 'raw_tone_mapping')\n",
    "    experiment_with_tone_mapping(jpeg_hdr, 'jpeg_tone_mapping')\n",
    "    \n",
    "    # 7. Find best tone mapping parameters\n",
    "    print(\"\\nStep 7: Find best tone mapping parameters for RAW HDR\")\n",
    "    raw_key, raw_burn, raw_gamma = find_best_tone_mapping_params(raw_hdr)\n",
    "    \n",
    "    print(\"\\nStep 8: Find best tone mapping parameters for JPEG HDR\")\n",
    "    jpeg_key, jpeg_burn, jpeg_gamma = find_best_tone_mapping_params(jpeg_hdr)\n",
    "    \n",
    "    # 9. Create final tone-mapped images with best parameters\n",
    "    print(\"\\nStep 9: Create final tone-mapped images\")\n",
    "    final_raw_ldr = apply_tone_mapping(raw_hdr, 'luminance', raw_key, raw_burn, raw_gamma)\n",
    "    final_jpeg_ldr = apply_tone_mapping(jpeg_hdr, 'luminance', jpeg_key, jpeg_burn, jpeg_gamma)\n",
    "    \n",
    "    # 10. Save final images\n",
    "    print(\"\\nStep 10: Save final images\")\n",
    "    cv2.imwrite('final_raw_tonemap.png', cv2.cvtColor((final_raw_ldr * 255).astype(np.uint8), cv2.COLOR_RGB2BGR))\n",
    "    cv2.imwrite('final_jpeg_tonemap.png', cv2.cvtColor((final_jpeg_ldr * 255).astype(np.uint8), cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "    print(\"\\nWorkflow complete!\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GunMNPRhBAXv"
   },
   "source": [
    "##### *Discuss the above-generated images, commenting on which one you like the best.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdZiia7DGPM8"
   },
   "source": [
    "### **REFERENCES**\n",
    "[1] P. E. Debevec and J. Malik. Recovering high dynamic range radiance maps from photographs.\n",
    "In Proceedings of the 24th Annual Conference on Computer Graphics and Interactive Techniques,\n",
    "SIGGRAPH ’97, pages 369–378, New York, NY, USA, 1997. ACM Press/Addison-Wesley Publishing\n",
    "Co.\n",
    "\n",
    "[2] E. Reinhard, M. Stark, P. Shirley, and J. Ferwerda. Photographic tone reproduction for digital\n",
    "images. In Proceedings of the 29th Annual Conference on Computer Graphics and Interactive\n",
    "Techniques, SIGGRAPH ’02, pages 267–276, New York, NY, USA, 2002. ACM.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
